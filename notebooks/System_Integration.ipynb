{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUUlNfJJF0jf"
      },
      "source": [
        "# üéì Optimized Exam Proctoring System (OEPS) v2.0 ‚Äî Colab Edition\n",
        "\n",
        "## Optimized Pipeline (No LSTM, No DeepSORT)\n",
        "\n",
        "```\n",
        "Stage 1: Video Input ‚Üí OpenCV Frame Extraction (every frame)\n",
        "Stage 2: Pose Estimation ‚Üí YOLOv11s-Pose (Detect + Extract 13 Keypoints)\n",
        "Stage 3: Lightweight IoU Tracking (Assign Track IDs)\n",
        "Stage 4: Skeleton ROI ‚Üí Generate Bones (224√ó224√ó3) ‚Äî IN MEMORY, no disk I/O\n",
        "Stage 5: Feature Extraction ‚Üí ResNet50V2 ‚Üí Binary Classification (BATCHED)\n",
        "Stage 6: Temporal Voting ‚Üí Sliding Window Majority Vote\n",
        "Stage 7: Output ‚Üí Color-Coded Boxes + Write to Video File at original FPS\n",
        "```\n",
        "\n",
        "### Speed Optimizations vs v1.0:\n",
        "- ‚ùå Removed LSTM (useless at low frame rates, sequence_length=2 = just counting)\n",
        "- ‚ùå Removed DeepSORT (overkill for few students, replaced with IoU tracker)\n",
        "- ‚úÖ `model(x, training=False)` instead of `model.predict()` ‚Äî 10-50x faster per call\n",
        "- ‚úÖ Batched ResNet inference (all students in one forward pass)\n",
        "- ‚úÖ Fixed preprocessing mismatch (now uses correct `resnet_v2.preprocess_input`)\n",
        "- ‚úÖ No matplotlib display during processing (write video ‚Üí watch after)\n",
        "- ‚úÖ No disk I/O during inference (skeleton images stay in memory)\n",
        "\n",
        "---"
      ],
      "id": "BUUlNfJJF0jf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0u4kl-fF0ji"
      },
      "source": [
        "## üì¶ 1. Installation and Setup"
      ],
      "id": "b0u4kl-fF0ji"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw3aqAKpF0jj",
        "outputId": "8be5e683-32b9-4a90-c2ba-0e326e527a98"
      },
      "source": [
        "# Install required packages\n",
        "!pip install -q -U ultralytics\n",
        "!pip install -q opencv-python-headless\n",
        "!pip install -q tensorflow\n",
        "!pip install -q numpy matplotlib seaborn\n",
        "\n",
        "print('‚úÖ All packages installed!')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ All packages installed!\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "cw3aqAKpF0jj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp7t09WUF0jl"
      },
      "source": [
        "## üìÅ 2. Platform Setup (Colab)\n",
        "\n",
        "Mount Google Drive to access your model and video files."
      ],
      "id": "Vp7t09WUF0jl"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcxR5fwAK399",
        "outputId": "8e549be7-85c8-4f64-ef45-dcf7eafba0d5"
      },
      "id": "BcxR5fwAK399",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVkc8rygF0jn"
      },
      "source": [
        "## üìö 3. Import Libraries"
      ],
      "id": "OVkc8rygF0jn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qfz6aoYWF0jn",
        "outputId": "ee665dbd-5a8a-4637-d33a-517d16b1f93b"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"‚úÖ GPU detected: {gpus[0].name}\")\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected! Running on CPU (will be slow)\")\n",
        "\n",
        "print(f\"TensorFlow: {tf.__version__}\")\n",
        "print(f\"OpenCV: {cv2.__version__}\")\n",
        "print(f\"TF GPU in use: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU detected: /physical_device:GPU:0\n",
            "TensorFlow: 2.19.0\n",
            "OpenCV: 4.13.0\n",
            "TF GPU in use: True\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "Qfz6aoYWF0jn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RqPUpMZF0jo"
      },
      "source": [
        "## ‚öôÔ∏è 4. Configuration"
      ],
      "id": "2RqPUpMZF0jo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E6ddvq9F0jp",
        "outputId": "e0450078-43de-4a15-f3fa-75c54604ab79"
      },
      "source": [
        "##############################################################################\n",
        "# CELL 8 ‚Äî REPLACE ENTIRE CELL (Config)\n",
        "#\n",
        "# Changes:\n",
        "#   1. Added UPPER_BODY_KPTS, KPT_CONF_THRESHOLD, bone thickness constants\n",
        "#   2. Added (0,5) and (0,6) neck connections to SKELETON_CONNECTIONS\n",
        "#   3. ROI_SIZE added for skeleton normalization\n",
        "##############################################################################\n",
        "\n",
        "class Config:\n",
        "    # === MODEL PATHS (UPDATE THESE) ===\n",
        "    RESNET_MODEL_PATH = \"/content/drive/MyDrive/Model/resnet50v2_final.keras\"\n",
        "    VIDEO_PATH = \"/content/drive/MyDrive/Video_Collection/Testing video's /test4.mp4\"\n",
        "    OUTPUT_DIR = \"/content\"\n",
        "\n",
        "    # Image processing\n",
        "    IMG_SIZE = (224, 224)\n",
        "    ROI_SIZE = 224\n",
        "\n",
        "    # Temporal voting\n",
        "    VOTE_WINDOW = 1\n",
        "    SUSPICIOUS_THRESHOLD = 0.5\n",
        "    VOTE_THRESHOLD = 1\n",
        "\n",
        "    # YOLO detection\n",
        "    POSE_CONF_THRESHOLD = 0.5\n",
        "    YOLO_POSE_MODEL = 'yolo11s-pose.pt'\n",
        "\n",
        "    # IoU tracker\n",
        "    IOU_THRESHOLD = 0.3\n",
        "    MAX_LOST_FRAMES = 30\n",
        "\n",
        "    # Skeleton parameters ‚Äî MUST match training extraction code\n",
        "    KPT_CONF_THRESHOLD = 0.4\n",
        "    BODY_BONE_THICKNESS = 3\n",
        "    HEAD_BONE_THICKNESS = 2\n",
        "\n",
        "    UPPER_BODY_KPTS = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}\n",
        "\n",
        "    KEYPOINT_NAMES = [\n",
        "        'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
        "        'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
        "        'left_wrist', 'right_wrist', 'left_hip', 'right_hip'\n",
        "    ]\n",
        "\n",
        "    # FIXED: 14 connections (old had 12 ‚Äî was missing neck bones)\n",
        "    SKELETON_CONNECTIONS = [\n",
        "        (0, 1), (0, 2), (1, 3), (2, 4),       # head\n",
        "        (0, 5), (0, 6),                         # neck  ‚Üê WERE MISSING\n",
        "        (5, 6),                                  # shoulders\n",
        "        (5, 7), (7, 9),                          # left arm\n",
        "        (6, 8), (8, 10),                         # right arm\n",
        "        (5, 11), (6, 12), (11, 12)               # torso\n",
        "    ]\n",
        "\n",
        "config = Config()\n",
        "print(\"‚úÖ Configuration loaded!\")\n",
        "print(f\"   ResNet model: {config.RESNET_MODEL_PATH}\")\n",
        "print(f\"   Video: {config.VIDEO_PATH}\")\n",
        "print(f\"   Skeleton connections: {len(config.SKELETON_CONNECTIONS)} (should be 14)\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuration loaded!\n",
            "   ResNet model: /content/drive/MyDrive/Model/resnet50v2_final.keras\n",
            "   Video: /content/drive/MyDrive/Video_Collection/Testing video's /test4.mp4\n",
            "   Skeleton connections: 14 (should be 14)\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "7E6ddvq9F0jp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoBthQtNF0jq"
      },
      "source": [
        "## ü§ñ 5. Load Models\n",
        "\n",
        "Only **2 models** now (was 3):\n",
        "1. **YOLOv11s-Pose** ‚Äî Pose estimation + keypoint extraction\n",
        "2. **ResNet50V2** ‚Äî Direct binary classification (normal vs suspicious)\n",
        "\n",
        "~~LSTM~~ ‚Äî Removed. Temporal voting handles smoothing."
      ],
      "id": "BoBthQtNF0jq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl2EaTdeF0jq",
        "outputId": "18bbb0a7-fcd2-4bf2-858d-5519fdde97ae"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import functools\n",
        "\n",
        "# Download YOLO if needed\n",
        "if not os.path.exists('yolo11s-pose.pt'):\n",
        "    print('Downloading yolo11s-pose.pt...')\n",
        "    !wget -q https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-pose.pt\n",
        "\n",
        "# PyTorch 2.6 security bypass\n",
        "torch.load = functools.partial(torch.load, weights_only=False)\n",
        "\n",
        "# === LOAD YOLO ===\n",
        "print(\"Loading YOLOv11s-Pose...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "yolo_model = YOLO('yolo11s-pose.pt').to(device)\n",
        "print(f\"YOLO on: {device}\")\n",
        "print(\"‚úÖ YOLO loaded!\")\n",
        "\n",
        "# === LOAD RESNET (as CLASSIFIER, not feature extractor) ===\n",
        "print(f\"Loading ResNet from {config.RESNET_MODEL_PATH}...\")\n",
        "if not os.path.exists(config.RESNET_MODEL_PATH):\n",
        "    raise FileNotFoundError(f\"Model not found: {config.RESNET_MODEL_PATH}\")\n",
        "\n",
        "resnet_model = load_model(config.RESNET_MODEL_PATH)\n",
        "print(f\"‚úÖ ResNet loaded! Output shape: {resnet_model.output_shape}\")\n",
        "\n",
        "# Warm up models\n",
        "print(\"\\nWarming up models...\")\n",
        "dummy_img = np.random.rand(1, 224, 224, 3).astype(np.float32)\n",
        "_ = resnet_model(dummy_img, training=False)\n",
        "print(\"‚úÖ ResNet warmed up!\")\n",
        "\n",
        "dummy_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
        "_ = yolo_model(dummy_frame, conf=0.5, verbose=False)\n",
        "print(\"‚úÖ YOLO warmed up!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ ALL MODELS LOADED AND READY!\")\n",
        "print(\"=\"*50)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading YOLOv11s-Pose...\n",
            "YOLO on: cuda\n",
            "‚úÖ YOLO loaded!\n",
            "Loading ResNet from /content/drive/MyDrive/Model/resnet50v2_final.keras...\n",
            "‚úÖ ResNet loaded! Output shape: (None, 1)\n",
            "\n",
            "Warming up models...\n",
            "‚úÖ ResNet warmed up!\n",
            "‚úÖ YOLO warmed up!\n",
            "\n",
            "==================================================\n",
            "‚úÖ ALL MODELS LOADED AND READY!\n",
            "==================================================\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "Jl2EaTdeF0jq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9ub5fjqF0jr"
      },
      "source": [
        "## ü¶¥ 6. Skeleton Generator (In-Memory, No Disk I/O)"
      ],
      "id": "S9ub5fjqF0jr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp85okdIF0jr",
        "outputId": "7ae81e0a-2abc-4959-f2bf-30768ec68edf"
      },
      "source": [
        "##############################################################################\n",
        "# CELL 12 ‚Äî REPLACE ENTIRE CELL (SkeletonGenerator)\n",
        "#\n",
        "# This is THE critical fix. Old code used YOLO bbox crop + resize.\n",
        "# New code matches training: center+scale normalization, grayscale,\n",
        "# bones only, dilation, variable thickness, validation.\n",
        "##############################################################################\n",
        "\n",
        "class SkeletonGenerator:\n",
        "    \"\"\"Generates skeleton images matching training pipeline exactly.\"\"\"\n",
        "\n",
        "    def __init__(self, roi_size=224):\n",
        "        self.roi_size = roi_size\n",
        "        self.dilate_kernel = np.ones((3, 3), np.uint8)\n",
        "\n",
        "    def is_valid_skeleton(self, conf):\n",
        "        \"\"\"Relaxed validation for inference ‚Äî more lenient than training.\"\"\"\n",
        "\n",
        "        th = config.KPT_CONF_THRESHOLD\n",
        "        # Count how many upper body keypoints are confident\n",
        "        visible = sum(1 for i in config.UPPER_BODY_KPTS if i < len(conf) and conf[i] > th)\n",
        "        # Need at least 5 visible keypoints AND at least one shoulder\n",
        "        return visible >= 5 and (conf[5] > th or conf[6] > th)\n",
        "\n",
        "    def create_skeleton_image(self, keypoints, bbox):\n",
        "        \"\"\"\n",
        "        Generate normalized skeleton image identical to training.\n",
        "        Returns 224x224 grayscale image, or None if invalid.\n",
        "        \"\"\"\n",
        "        if len(keypoints) < 13:\n",
        "            return None\n",
        "\n",
        "        xy = keypoints[:, :2]\n",
        "        conf = keypoints[:, 2]\n",
        "\n",
        "        if not self.is_valid_skeleton(conf):\n",
        "            return None\n",
        "\n",
        "        # Collect valid upper body points\n",
        "        valid_pts = [\n",
        "            xy[i] for i in config.UPPER_BODY_KPTS\n",
        "            if i < len(conf) and conf[i] > config.KPT_CONF_THRESHOLD\n",
        "        ]\n",
        "        if len(valid_pts) <3:\n",
        "            return None\n",
        "\n",
        "        pts = np.array(valid_pts)\n",
        "        min_x, min_y = pts.min(axis=0)\n",
        "        max_x, max_y = pts.max(axis=0)\n",
        "        skel_w = max_x - min_x\n",
        "        skel_h = max_y - min_y\n",
        "        if max(skel_w, skel_h) < 1e-6:\n",
        "            return None\n",
        "\n",
        "        # Center + scale to fill 90% of canvas (same as training)\n",
        "        scale = 0.9 * self.roi_size / max(skel_w, skel_h)\n",
        "        cx = (min_x + max_x) / 2\n",
        "        cy = (min_y + max_y) / 2\n",
        "\n",
        "        # Grayscale canvas (same as training)\n",
        "        skeleton_img = np.zeros((self.roi_size, self.roi_size), dtype=np.uint8)\n",
        "\n",
        "        def norm(pt):\n",
        "            x = (pt[0] - cx) * scale + self.roi_size / 2\n",
        "            y = (pt[1] - cy) * scale + self.roi_size / 2\n",
        "            return int(x), int(y)\n",
        "\n",
        "        # Bones only ‚Äî NO circles (same as training)\n",
        "        for i, j in config.SKELETON_CONNECTIONS:\n",
        "            if i >= len(conf) or j >= len(conf):\n",
        "                continue\n",
        "            if conf[i] < config.KPT_CONF_THRESHOLD or conf[j] < config.KPT_CONF_THRESHOLD:\n",
        "                continue\n",
        "            if i not in config.UPPER_BODY_KPTS or j not in config.UPPER_BODY_KPTS:\n",
        "                continue\n",
        "            p1 = norm(xy[i])\n",
        "            p2 = norm(xy[j])\n",
        "            if not (0 <= p1[0] < self.roi_size and 0 <= p1[1] < self.roi_size and\n",
        "                    0 <= p2[0] < self.roi_size and 0 <= p2[1] < self.roi_size):\n",
        "                continue\n",
        "            thickness = config.HEAD_BONE_THICKNESS if (i == 0 or j == 0) else config.BODY_BONE_THICKNESS\n",
        "            cv2.line(skeleton_img, p1, p2, 255, thickness)\n",
        "\n",
        "        # Dilation (same as training)\n",
        "        skeleton_img = cv2.dilate(skeleton_img, self.dilate_kernel, iterations=1)\n",
        "        return skeleton_img\n",
        "\n",
        "    def preprocess_batch(self, skeleton_images):\n",
        "        \"\"\"\n",
        "        Match Keras ImageDataGenerator behavior:\n",
        "        grayscale ‚Üí RGB (3-channel) ‚Üí resnet_v2.preprocess_input\n",
        "        \"\"\"\n",
        "        batch = []\n",
        "        for img in skeleton_images:\n",
        "            img_3ch = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "            batch.append(img_3ch.astype(np.float32))\n",
        "        return resnet_preprocess(np.array(batch))\n",
        "\n",
        "skeleton_gen = SkeletonGenerator()\n",
        "print(\"‚úÖ Skeleton generator ready (FIXED ‚Äî matches training pipeline)\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Skeleton generator ready (FIXED ‚Äî matches training pipeline)\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "tp85okdIF0jr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxRSee-hF0jr"
      },
      "source": [
        "## üéØ 7. Lightweight IoU Tracker (Replaces DeepSORT)\n",
        "\n",
        "For 3 students in a fixed camera, simple IoU matching is sufficient and much faster."
      ],
      "id": "fxRSee-hF0jr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggr-UbQ1F0jr",
        "outputId": "4bac1fa0-50fd-4bcc-bea4-36a254cb2eab"
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "class SimpleIoUTracker:\n",
        "    \"\"\"\n",
        "    Lightweight tracker using IoU matching.\n",
        "    No deep features, no Kalman filter ‚Äî just bounding box overlap.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, iou_threshold=0.3, max_lost=30):\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.max_lost = max_lost\n",
        "        self.tracks = {}\n",
        "        self.next_id = 1\n",
        "        self.vote_history = {}\n",
        "        self.colors = {}\n",
        "\n",
        "    def _iou(self, boxA, boxB):\n",
        "        xA = max(boxA[0], boxB[0])\n",
        "        yA = max(boxA[1], boxB[1])\n",
        "        xB = min(boxA[2], boxB[2])\n",
        "        yB = min(boxA[3], boxB[3])\n",
        "        inter = max(0, xB - xA) * max(0, yB - yA)\n",
        "        areaA = (boxA[2]-boxA[0]) * (boxA[3]-boxA[1])\n",
        "        areaB = (boxB[2]-boxB[0]) * (boxB[3]-boxB[1])\n",
        "        union = areaA + areaB - inter\n",
        "        return inter / union if union > 0 else 0\n",
        "\n",
        "    def update(self, detections):\n",
        "        det_bboxes = [d[0] for d in detections]\n",
        "        matched = []\n",
        "        used_dets = set()\n",
        "        used_tracks = set()\n",
        "\n",
        "        if self.tracks and detections:\n",
        "            iou_matrix = []\n",
        "            track_ids = list(self.tracks.keys())\n",
        "            for tid in track_ids:\n",
        "                row = [self._iou(self.tracks[tid]['bbox'], db) for db in det_bboxes]\n",
        "                iou_matrix.append(row)\n",
        "\n",
        "            iou_matrix = np.array(iou_matrix) if iou_matrix else np.array([])\n",
        "            if iou_matrix.size > 0:\n",
        "                while True:\n",
        "                    max_val = iou_matrix.max()\n",
        "                    if max_val < self.iou_threshold:\n",
        "                        break\n",
        "                    ti, di = np.unravel_index(iou_matrix.argmax(), iou_matrix.shape)\n",
        "                    tid = track_ids[ti]\n",
        "                    self.tracks[tid]['bbox'] = det_bboxes[di]\n",
        "                    self.tracks[tid]['lost'] = 0\n",
        "                    matched.append((tid, det_bboxes[di], detections[di][2]))\n",
        "                    used_dets.add(di)\n",
        "                    used_tracks.add(tid)\n",
        "                    iou_matrix[ti, :] = -1\n",
        "                    iou_matrix[:, di] = -1\n",
        "\n",
        "        for di, det in enumerate(detections):\n",
        "            if di not in used_dets:\n",
        "                tid = self.next_id\n",
        "                self.next_id += 1\n",
        "                self.tracks[tid] = {'bbox': det[0], 'lost': 0}\n",
        "                self.vote_history[tid] = deque(maxlen=config.VOTE_WINDOW)\n",
        "                matched.append((tid, det[0], det[2]))\n",
        "\n",
        "        for tid in list(self.tracks.keys()):\n",
        "            if tid not in used_tracks and tid in self.tracks:\n",
        "                self.tracks[tid]['lost'] += 1\n",
        "                if self.tracks[tid]['lost'] > self.max_lost:\n",
        "                    del self.tracks[tid]\n",
        "                    self.vote_history.pop(tid, None)\n",
        "                    self.colors.pop(tid, None)\n",
        "\n",
        "        return matched\n",
        "\n",
        "    def add_vote(self, track_id, is_suspicious):\n",
        "        if track_id not in self.vote_history:\n",
        "            self.vote_history[track_id] = deque(maxlen=config.VOTE_WINDOW)\n",
        "        self.vote_history[track_id].append(int(is_suspicious))\n",
        "\n",
        "    def get_decision(self, track_id):\n",
        "        if track_id not in self.vote_history or len(self.vote_history[track_id]) == 0:\n",
        "            return 'Normal', 0.0\n",
        "        history = self.vote_history[track_id]\n",
        "        suspicious_count = sum(history)\n",
        "        ratio = suspicious_count / len(history)\n",
        "        if suspicious_count >= config.VOTE_THRESHOLD:\n",
        "            return 'Suspicious', ratio\n",
        "        return 'Normal', ratio\n",
        "\n",
        "    def get_color(self, track_id, label):\n",
        "        if label == 'Suspicious':\n",
        "            self.colors[track_id] = (0, 0, 255)\n",
        "        elif label == 'Normal':\n",
        "            self.colors[track_id] = (0, 255, 0)\n",
        "        return self.colors.get(track_id, (0, 255, 255))\n",
        "\n",
        "print(\"‚úÖ IoU Tracker ready (replaces DeepSORT)\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ IoU Tracker ready (replaces DeepSORT)\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "ggr-UbQ1F0jr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEfXUvCWF0js"
      },
      "source": [
        "## üîÑ 8. Optimized Processing Pipeline\n",
        "\n",
        "Key speed improvements:\n",
        "- `model(x, training=False)` instead of `model.predict()` ‚Äî **10-50x faster** per call\n",
        "- **Batched** ResNet inference ‚Äî all students in one forward pass\n",
        "- **Correct** preprocessing matching training\n",
        "- No LSTM ‚Äî direct ResNet classification + temporal voting"
      ],
      "id": "OEfXUvCWF0js"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVYCxVmvF0js",
        "outputId": "bca90be1-e8fe-4365-877a-5463b0dd0828"
      },
      "source": [
        "\n",
        "class OptimizedPipeline:\n",
        "    def __init__(self, yolo_model, resnet_model, skeleton_gen):\n",
        "        self.yolo = yolo_model\n",
        "        self.resnet = resnet_model\n",
        "        self.skeleton_gen = skeleton_gen\n",
        "        self.tracker = SimpleIoUTracker(\n",
        "            iou_threshold=config.IOU_THRESHOLD,\n",
        "            max_lost=config.MAX_LOST_FRAMES\n",
        "        )\n",
        "        self.frame_count = 0\n",
        "        self.total_suspicious = 0\n",
        "        self.skipped_invalid = 0\n",
        "        self.timing = {'yolo': [], 'skeleton': [], 'resnet': [], 'tracking': [], 'total': []}\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        t_total = time.perf_counter()\n",
        "        self.frame_count += 1\n",
        "\n",
        "        # === STAGE 1: YOLO Pose Detection ===\n",
        "        t0 = time.perf_counter()\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = self.yolo(rgb_frame, conf=config.POSE_CONF_THRESHOLD, verbose=False, device=0)\n",
        "        self.timing['yolo'].append(time.perf_counter() - t0)\n",
        "\n",
        "        detections = []\n",
        "        if results[0].keypoints is not None and results[0].boxes is not None:\n",
        "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "            confs = results[0].boxes.conf.cpu().numpy()\n",
        "            kpts = results[0].keypoints.data.cpu().numpy()\n",
        "            for b, c, k in zip(boxes, confs, kpts):\n",
        "                detections.append((b, c, k))\n",
        "\n",
        "        # === STAGE 2: IoU Tracking ===\n",
        "        t0 = time.perf_counter()\n",
        "        matched_tracks = self.tracker.update(detections)\n",
        "        self.timing['tracking'].append(time.perf_counter() - t0)\n",
        "\n",
        "        if not matched_tracks:\n",
        "            self.timing['skeleton'].append(0)\n",
        "            self.timing['resnet'].append(0)\n",
        "            self.timing['total'].append(time.perf_counter() - t_total)\n",
        "            return frame, []\n",
        "\n",
        "        # === STAGE 3: Generate skeleton images ===\n",
        "        t0 = time.perf_counter()\n",
        "        skeleton_batch = []\n",
        "        track_info = []\n",
        "        skipped_tracks = []\n",
        "\n",
        "        for track_id, bbox, keypoints in matched_tracks:\n",
        "            skeleton_img = self.skeleton_gen.create_skeleton_image(keypoints, bbox)\n",
        "            if skeleton_img is None:\n",
        "                self.skipped_invalid += 1\n",
        "                skipped_tracks.append((track_id, bbox))\n",
        "                continue\n",
        "            skeleton_batch.append(skeleton_img)\n",
        "            track_info.append((track_id, bbox))\n",
        "\n",
        "        self.timing['skeleton'].append(time.perf_counter() - t0)\n",
        "\n",
        "        detections_info = []\n",
        "\n",
        "        # === STAGE 4: ResNet Classification ===\n",
        "        if skeleton_batch:\n",
        "            t0 = time.perf_counter()\n",
        "            batch_preprocessed = self.skeleton_gen.preprocess_batch(skeleton_batch)\n",
        "            with tf.device('/GPU:0'):\n",
        "                batch_tensor = tf.constant(batch_preprocessed, dtype=tf.float32)\n",
        "                predictions = self.resnet(batch_tensor, training=False).numpy().flatten()\n",
        "            self.timing['resnet'].append(time.perf_counter() - t0)\n",
        "\n",
        "            # === STAGE 5: Temporal Voting + Draw for classified students ===\n",
        "            for i, (track_id, bbox) in enumerate(track_info):\n",
        "                prob = float(predictions[i]) if i < len(predictions) else 0.0\n",
        "                is_suspicious = 1 if prob >= config.SUSPICIOUS_THRESHOLD else 0\n",
        "                self.tracker.add_vote(track_id, is_suspicious)\n",
        "\n",
        "                label, vote_ratio = self.tracker.get_decision(track_id)\n",
        "                color = self.tracker.get_color(track_id, label)\n",
        "\n",
        "                if label == 'Suspicious':\n",
        "                    self.total_suspicious += 1\n",
        "\n",
        "                x1, y1, x2, y2 = map(int, bbox)\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "                label_text = f\"ID:{track_id} | {label} ({prob:.2f})\"\n",
        "                (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "                cv2.rectangle(frame, (x1, y1 - 30), (x1 + tw + 10, y1), color, -1)\n",
        "                cv2.putText(frame, label_text, (x1 + 5, y1 - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "                detections_info.append({\n",
        "                    'student_id': track_id,\n",
        "                    'label': label,\n",
        "                    'resnet_prob': prob,\n",
        "                    'vote_ratio': vote_ratio\n",
        "                })\n",
        "        else:\n",
        "            self.timing['resnet'].append(0)\n",
        "\n",
        "        # === STAGE 6: Draw skipped students with last known label ===\n",
        "        for track_id, bbox in skipped_tracks:\n",
        "            label, vote_ratio = self.tracker.get_decision(track_id)\n",
        "            color = self.tracker.get_color(track_id, label)\n",
        "            x1, y1, x2, y2 = map(int, bbox)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "            label_text = f\"ID:{track_id} | {label} (skip)\"\n",
        "            (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "            cv2.rectangle(frame, (x1, y1 - 30), (x1 + tw + 10, y1), color, -1)\n",
        "            cv2.putText(frame, label_text, (x1 + 5, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "            detections_info.append({\n",
        "                'student_id': track_id,\n",
        "                'label': label,\n",
        "                'resnet_prob': -1,\n",
        "                'vote_ratio': vote_ratio\n",
        "            })\n",
        "\n",
        "        total_students = len(track_info) + len(skipped_tracks)\n",
        "        info = f\"Frame: {self.frame_count} | Students: {total_students} | Suspicious: {self.total_suspicious}\"\n",
        "        cv2.putText(frame, info, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "        self.timing['total'].append(time.perf_counter() - t_total)\n",
        "        return frame, detections_info\n",
        "\n",
        "    def print_timing_stats(self):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"‚è±Ô∏è  PIPELINE TIMING BREAKDOWN (averages)\")\n",
        "        print(\"=\"*60)\n",
        "        for stage, times in self.timing.items():\n",
        "            if times:\n",
        "                avg_ms = np.mean(times) * 1000\n",
        "                print(f\"   {stage:12s}: {avg_ms:7.1f} ms\")\n",
        "        if self.timing['total']:\n",
        "            avg_total = np.mean(self.timing['total'])\n",
        "            print(f\"   {'FPS':12s}: {1.0/avg_total:7.1f}\")\n",
        "        print(f\"   Skipped (invalid skeleton): {self.skipped_invalid}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "print(\"‚úÖ Optimized pipeline ready!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Optimized pipeline ready!\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "eVYCxVmvF0js"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaInTzjrF0jt"
      },
      "source": [
        "## üß™ 9. Quick Test"
      ],
      "id": "zaInTzjrF0jt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-AwIJ8cF0jt",
        "outputId": "a43c0932-d138-4281-c272-32de4e9a708f"
      },
      "source": [
        "# Test the pipeline with a dummy frame\n",
        "pipeline = OptimizedPipeline(yolo_model, resnet_model, skeleton_gen)\n",
        "\n",
        "test_frame = np.random.randint(0, 255, (720, 1280, 3), dtype=np.uint8)\n",
        "processed, dets = pipeline.process_frame(test_frame)\n",
        "print(f\"‚úÖ Pipeline test passed! Detections: {len(dets)}\")\n",
        "print(f\"   Output frame shape: {processed.shape}\")\n",
        "\n",
        "for _ in range(5):\n",
        "    _, _ = pipeline.process_frame(test_frame)\n",
        "pipeline.print_timing_stats()\n",
        "print(\"\\n(Timing on random frames ‚Äî real video will differ)\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Pipeline test passed! Detections: 0\n",
            "   Output frame shape: (720, 1280, 3)\n",
            "\n",
            "============================================================\n",
            "‚è±Ô∏è  PIPELINE TIMING BREAKDOWN (averages)\n",
            "============================================================\n",
            "   yolo        :    19.2 ms\n",
            "   skeleton    :     0.0 ms\n",
            "   resnet      :     0.0 ms\n",
            "   tracking    :     0.0 ms\n",
            "   total       :    19.3 ms\n",
            "   FPS         :    51.8\n",
            "   Skipped (invalid skeleton): 0\n",
            "============================================================\n",
            "\n",
            "(Timing on random frames ‚Äî real video will differ)\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "1-AwIJ8cF0jt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTbnjKSPF0jt"
      },
      "source": [
        "## üé¨ 10. Process Video ‚Üí Save Output Video\n",
        "\n",
        "Processes every frame and writes to an output video at the original FPS.\n",
        "No real-time display ‚Äî watch the output video after."
      ],
      "id": "XTbnjKSPF0jt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTbnitOEF0jt",
        "outputId": "89965429-e3c8-4cba-d0bd-c3055eb327bb"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import time\n",
        "\n",
        "def save_processed_video_exact_match(video_path, output_path, process_every_n=10):\n",
        "    \"\"\"\n",
        "    Saves the video with the EXACT visual style and logic as your 18 FPS preview.\n",
        "    Uses 'OptimizedPipeline' and 'resnet_prob' as per your original code.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"‚ùå Source video not found: {video_path}\")\n",
        "        return\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # 1. Initialize Video Writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # --- LINE 24 FIX: Using your exact Pipeline class ---\n",
        "    pipe = OptimizedPipeline(yolo_model, resnet_model, skeleton_gen)\n",
        "    cached_annotations = []\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üíæ EXACT-MATCH SAVING STARTED\")\n",
        "    print(f\"üìÅ Target: {output_path}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    frame_num = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_num += 1\n",
        "\n",
        "        # Logic exactly from your preview loop\n",
        "        if frame_num % process_every_n == 1 or process_every_n == 1:\n",
        "            processed_frame, dets = pipe.process_frame(frame.copy())\n",
        "\n",
        "            cached_annotations = []\n",
        "            for d in dets:\n",
        "                track_id = d['student_id']\n",
        "                label = d['label']\n",
        "                prob = d['resnet_prob'] # Using your specific key name\n",
        "\n",
        "                # Matching your bbox retrieval logic\n",
        "                if track_id in pipe.tracker.tracks:\n",
        "                    bbox = pipe.tracker.tracks[track_id]['bbox']\n",
        "                else:\n",
        "                    bbox = None\n",
        "\n",
        "                color = pipe.tracker.get_color(track_id, label)\n",
        "                if bbox is not None:\n",
        "                    cached_annotations.append((bbox, label, prob, color, track_id))\n",
        "\n",
        "        # --- Replicating 'draw_annotations' and 'info' overlay ---\n",
        "        save_frame = frame.copy()\n",
        "\n",
        "        # Draw Annotations\n",
        "        for bbox, label, prob, color, track_id in cached_annotations:\n",
        "            x1, y1, x2, y2 = map(int, bbox)\n",
        "            cv2.rectangle(save_frame, (x1, y1), (x2, y2), color, 2)\n",
        "            label_text = f\"ID:{track_id} | {label} ({prob:.2f})\"\n",
        "            (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "            cv2.rectangle(save_frame, (x1, y1 - 30), (x1 + tw + 10, y1), color, -1)\n",
        "            cv2.putText(save_frame, label_text, (x1 + 5, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "        # Draw Info Bar (using pipe.total_suspicious)\n",
        "        info = f\"Frame: {frame_num} | Students: {len(cached_annotations)} | Suspicious: {pipe.total_suspicious}\"\n",
        "        cv2.putText(save_frame, info, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "        # Write the full-quality frame\n",
        "        out.write(save_frame)\n",
        "\n",
        "        if frame_num % 100 == 0:\n",
        "            print(f\"Processed {frame_num}/{total_frames} frames...\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"\\n‚úÖ Video saved successfully: {output_path}\")\n",
        "\n",
        "# --- RUN IT IN A NEW CELL ---\n",
        "final_output = os.path.join(config.OUTPUT_DIR, \"proctored_exact_match.mp4\")\n",
        "save_processed_video_exact_match(config.VIDEO_PATH, final_output, process_every_n=10)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üíæ EXACT-MATCH SAVING STARTED\n",
            "üìÅ Target: /content/proctored_exact_match.mp4\n",
            "============================================================\n",
            "Processed 100/1807 frames...\n",
            "Processed 200/1807 frames...\n",
            "Processed 300/1807 frames...\n",
            "Processed 400/1807 frames...\n",
            "Processed 500/1807 frames...\n",
            "Processed 600/1807 frames...\n",
            "Processed 700/1807 frames...\n",
            "Processed 800/1807 frames...\n",
            "Processed 900/1807 frames...\n",
            "Processed 1000/1807 frames...\n",
            "Processed 1100/1807 frames...\n",
            "Processed 1200/1807 frames...\n",
            "Processed 1300/1807 frames...\n",
            "Processed 1400/1807 frames...\n",
            "Processed 1500/1807 frames...\n",
            "Processed 1600/1807 frames...\n",
            "Processed 1700/1807 frames...\n",
            "Processed 1800/1807 frames...\n",
            "\n",
            "‚úÖ Video saved successfully: /content/proctored_exact_match.mp4\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "rTbnitOEF0jt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dokY-z0FF0ju"
      },
      "source": [
        "## üñ•Ô∏è 10b. Process Video with Live Preview (Fast Display)\n",
        "\n",
        "Displays every frame with cached annotations for smooth playback.\n",
        "Only processes every Nth frame through YOLO+ResNet. No video file saved."
      ],
      "id": "dokY-z0FF0ju"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAX7SK2XF0ju",
        "outputId": "3c2a5ba3-87db-4135-ff64-7994cf742891"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import base64\n",
        "from IPython.display import display, HTML\n",
        "import time\n",
        "import os\n",
        "\n",
        "def process_video_smooth_preview(video_path, process_every_n=10, preview_width=480):\n",
        "    \"\"\"\n",
        "    Process every Nth frame through YOLO+ResNet, but DISPLAY every frame\n",
        "    with the last processed frame's labels applied to fresh pixels.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"‚ùå Video not found: {video_path}\")\n",
        "        return None\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = total_frames / fps if fps > 0 else 0\n",
        "    preview_height = int(preview_width * height / width)\n",
        "    frame_delay = 1.0 / fps if fps > 0 else 1.0 / 30.0\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìπ Input: {width}x{height} @ {fps:.1f} FPS | {total_frames} frames | {duration:.1f}s\")\n",
        "    print(f\"üìù Processing every {process_every_n} frame(s), displaying ALL frames\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    pipe = OptimizedPipeline(yolo_model, resnet_model, skeleton_gen)\n",
        "    cached_annotations = []\n",
        "\n",
        "    def draw_annotations(frame, annotations):\n",
        "        for bbox, label, prob, color, track_id in annotations:\n",
        "            x1, y1, x2, y2 = map(int, bbox)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "            label_text = f\"ID:{track_id} | {label} ({prob:.2f})\"\n",
        "            (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "            cv2.rectangle(frame, (x1, y1 - 30), (x1 + tw + 10, y1), color, -1)\n",
        "            cv2.putText(frame, label_text, (x1 + 5, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "        return frame\n",
        "\n",
        "    frame_num = 0\n",
        "    processed_count = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    display_handle = display(HTML(\"\"), display_id=True)\n",
        "\n",
        "    while True:\n",
        "        loop_start = time.perf_counter()\n",
        "\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_num += 1\n",
        "\n",
        "        if frame_num % process_every_n == 1 or process_every_n == 1:\n",
        "            processed_frame, dets = pipe.process_frame(frame.copy())\n",
        "            processed_count += 1\n",
        "\n",
        "            cached_annotations = []\n",
        "            for d in dets:\n",
        "                track_id = d['student_id']\n",
        "                label = d['label']\n",
        "                prob = d['resnet_prob']\n",
        "                if track_id in pipe.tracker.tracks:\n",
        "                    bbox = pipe.tracker.tracks[track_id]['bbox']\n",
        "                else:\n",
        "                    bbox = None\n",
        "                color = pipe.tracker.get_color(track_id, label)\n",
        "                if bbox is not None:\n",
        "                    cached_annotations.append((bbox, label, prob, color, track_id))\n",
        "\n",
        "        display_frame = frame.copy()\n",
        "        draw_annotations(display_frame, cached_annotations)\n",
        "\n",
        "        info = f\"Frame: {frame_num} | Students: {len(cached_annotations)} | Suspicious: {pipe.total_suspicious}\"\n",
        "        cv2.putText(display_frame, info, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "        preview = cv2.resize(display_frame, (preview_width, preview_height))\n",
        "        _, jpeg = cv2.imencode('.jpg', preview, [cv2.IMWRITE_JPEG_QUALITY, 70])\n",
        "        b64 = base64.b64encode(jpeg.tobytes()).decode('utf-8')\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        display_fps = frame_num / elapsed if elapsed > 0 else 0\n",
        "        progress = frame_num / total_frames * 100\n",
        "        mins = int(elapsed) // 60\n",
        "        secs = int(elapsed) % 60\n",
        "\n",
        "        html = f\"\"\"\n",
        "        <div style=\"font-family: monospace; background: #1a1a1a; padding: 10px; border-radius: 8px; display: inline-block;\">\n",
        "            <img src=\"data:image/jpeg;base64,{b64}\" style=\"border-radius: 4px;\"/>\n",
        "            <div style=\"color: #00ff88; margin-top: 6px; font-size: 14px;\">\n",
        "                Frame: {frame_num}/{total_frames} |\n",
        "                Progress: {progress:.1f}% |\n",
        "                Output FPS: {display_fps:.1f} |\n",
        "                Time: {mins:02d}:{secs:02d} |\n",
        "                Students: {len(cached_annotations)} |\n",
        "                Suspicious: {pipe.total_suspicious}\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display_handle.update(HTML(html))\n",
        "\n",
        "        elapsed_frame = time.perf_counter() - loop_start\n",
        "        wait = frame_delay - elapsed_frame\n",
        "        if wait > 0:\n",
        "            time.sleep(wait)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    final_display_fps = frame_num / total_time if total_time > 0 else 0\n",
        "    total_mins = int(total_time) // 60\n",
        "    total_secs = int(total_time) % 60\n",
        "\n",
        "    display_handle.update(HTML(\n",
        "        f'<div style=\"font-family:monospace; color:#00ff88; font-size:16px; padding:10px;\">'\n",
        "        f'‚úÖ DONE! {frame_num} frames | '\n",
        "        f'Output: {final_display_fps:.1f} FPS | '\n",
        "        f'Total Time: {total_mins:02d}:{total_secs:02d} | '\n",
        "        f'Suspicious: {pipe.total_suspicious}</div>'\n",
        "    ))\n",
        "\n",
        "    pipe.print_timing_stats()\n",
        "\n",
        "    return {\n",
        "        'total_frames': frame_num,\n",
        "        'processed_frames': processed_count,\n",
        "        'total_time': total_time,\n",
        "        'display_fps': final_display_fps,\n",
        "        'suspicious_count': pipe.total_suspicious,\n",
        "        'pipeline': pipe\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Smooth preview function ready!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Smooth preview function ready!\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "mAX7SK2XF0ju"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQOHavWRF0jv"
      },
      "source": [
        "## ‚ñ∂Ô∏è 11. Run Live Preview"
      ],
      "id": "wQOHavWRF0jv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "RjD93xb1F0jv",
        "outputId": "10d54972-aef4-44aa-b52f-660016fcf06f"
      },
      "source": [
        "# === LIVE PREVIEW (no file saved) ===\n",
        "VIDEO_PATH = config.VIDEO_PATH\n",
        "\n",
        "results = process_video_smooth_preview(\n",
        "    video_path=VIDEO_PATH,\n",
        "    process_every_n=10,\n",
        "    preview_width=480\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üìπ Input: 1280x720 @ 30.0 FPS | 1793 frames | 59.8s\n",
            "üìù Processing every 10 frame(s), displaying ALL frames\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"font-family:monospace; color:#00ff88; font-size:16px; padding:10px;\">‚úÖ DONE! 1793 frames | Output: 17.5 FPS | Total Time: 01:42 | Suspicious: 216</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "‚è±Ô∏è  PIPELINE TIMING BREAKDOWN (averages)\n",
            "============================================================\n",
            "   yolo        :    20.3 ms\n",
            "   skeleton    :     0.6 ms\n",
            "   resnet      :   241.9 ms\n",
            "   tracking    :     0.2 ms\n",
            "   total       :   264.0 ms\n",
            "   FPS         :     3.8\n",
            "   Skipped (invalid skeleton): 0\n",
            "============================================================\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "RjD93xb1F0jv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7Qjtpj_F0jw"
      },
      "source": [
        "## üìä 14. Results Analysis"
      ],
      "id": "q7Qjtpj_F0jw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGKNtfCvF0jx"
      },
      "source": [
        "def generate_report(results):\n",
        "    if not results:\n",
        "        print(\"No results to analyze.\")\n",
        "        return\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"üìä EXAM PROCTORING ANALYSIS REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"   Frames processed: {results['processed_frames']}/{results['total_frames']}\")\n",
        "    print(f\"   Processing speed: {results.get('avg_fps', results.get('display_fps', 0)):.1f} FPS\")\n",
        "    print(f\"   Total time: {results['total_time']:.1f}s\")\n",
        "    print(f\"   Suspicious events: {results['suspicious_count']}\")\n",
        "\n",
        "    fps = results.get('avg_fps', results.get('display_fps', 0))\n",
        "    if fps >= 30:\n",
        "        print(\"\\n   üü¢ REAL-TIME CAPABLE (‚â•30 FPS)\")\n",
        "    elif fps >= 15:\n",
        "        print(\"\\n   üü° NEAR REAL-TIME (15-30 FPS)\")\n",
        "    else:\n",
        "        print(f\"\\n   üî¥ BELOW REAL-TIME ({fps:.0f} FPS)\")\n",
        "        recommended_skip = max(1, int(30 / fps)) if fps > 0 else 10\n",
        "        print(f\"   üí° Recommendation: use process_every_n={recommended_skip}\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if results:\n",
        "    generate_report(results)"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "MGKNtfCvF0jx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub5PCzRUF0jx"
      },
      "source": [
        "## üìä 14b. Full Pipeline Evaluation (Ground Truth Comparison)\n",
        "\n",
        "Upload a `ground_truth.csv` to your Google Drive with format:\n",
        "```\n",
        "frame_start,frame_end,student_id,label\n",
        "1,240,1,normal\n",
        "241,450,1,suspicious\n",
        "```"
      ],
      "id": "ub5PCzRUF0jx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eYouSdWF0jx"
      },
      "source": [
        "import csv\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def evaluate_pipeline_on_video(video_path, ground_truth_csv, process_every_n=1):\n",
        "    \"\"\"Compare pipeline predictions against manually annotated ground truth.\"\"\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"‚ùå Video not found: {video_path}\")\n",
        "        return\n",
        "    if not os.path.exists(ground_truth_csv):\n",
        "        print(f\"‚ùå Ground truth CSV not found: {ground_truth_csv}\")\n",
        "        return\n",
        "\n",
        "    gt = {}\n",
        "    with open(ground_truth_csv, 'r') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            start = int(row['frame_start'])\n",
        "            end = int(row['frame_end'])\n",
        "            sid = int(row['student_id'])\n",
        "            label = row['label'].strip().lower()\n",
        "            for frame in range(start, end + 1):\n",
        "                gt[(frame, sid)] = label\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    pipe = OptimizedPipeline(yolo_model, resnet_model, skeleton_gen)\n",
        "\n",
        "    frame_num = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"Evaluating {total_frames} frames...\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_num += 1\n",
        "\n",
        "        if frame_num % process_every_n == 1 or process_every_n == 1:\n",
        "            _, dets = pipe.process_frame(frame)\n",
        "\n",
        "            for d in dets:\n",
        "                key = (frame_num, d['student_id'])\n",
        "                if key in gt:\n",
        "                    true_label = 1 if gt[key] == 'suspicious' else 0\n",
        "                    pred_label = 1 if d['label'] == 'Suspicious' else 0\n",
        "                    y_true.append(true_label)\n",
        "                    y_pred.append(pred_label)\n",
        "\n",
        "        if frame_num % 200 == 0:\n",
        "            print(f\"  Frame {frame_num}/{total_frames}...\")\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if not y_true:\n",
        "        print(\"‚ùå No matching ground truth found. Check your student IDs match the tracker IDs.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìä FULL PIPELINE EVALUATION RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"   Total matched predictions: {len(y_true)}\")\n",
        "    print()\n",
        "    print(classification_report(y_true, y_pred, target_names=[\"Normal\", \"Suspicious\"]))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# === USAGE ===\n",
        "# 1. First run live preview to see which track IDs match which students\n",
        "# 2. Create ground_truth.csv and upload to Google Drive\n",
        "# 3. Uncomment and run:\n",
        "#\n",
        "# evaluate_pipeline_on_video(\n",
        "#     video_path=config.VIDEO_PATH,\n",
        "#     ground_truth_csv=os.path.join(MODEL_DIR, 'ground_truth.csv'),\n",
        "#     process_every_n=1\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "5eYouSdWF0jx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ9_mfGmF0jy"
      },
      "source": [
        "## üìñ 16. System Usage Guide\n",
        "\n",
        "### Architecture (v2.0 ‚Äî Optimized):\n",
        "\n",
        "| Component | Purpose | Speed Impact |\n",
        "|-----------|---------|-------------|\n",
        "| **YOLOv11s-Pose** | Detects students, extracts 13 keypoints | ~15-25ms |\n",
        "| **IoU Tracker** | Tracks students across frames | <1ms |\n",
        "| **Skeleton Generator** | Creates 224√ó224 skeleton images in memory | ~2-5ms |\n",
        "| **ResNet50V2** | Classifies normal vs suspicious (batched) | ~5-15ms |\n",
        "| **Temporal Voting** | Majority vote over last N predictions | <0.1ms |\n",
        "\n",
        "### Output:\n",
        "- üü© **Green Box**: Normal behavior\n",
        "- üü• **Red Box**: Suspicious\n",
        "- üü® **Yellow Box**: New student (not enough votes yet)\n",
        "- **Label**: `ID:X | Label (ResNet_probability)`\n",
        "\n",
        "### Tips:\n",
        "1. Run benchmark first to know your actual FPS\n",
        "2. Use `process_every_n=1` if FPS ‚â• 30, otherwise increase N\n",
        "3. Output video always plays at original FPS regardless of processing speed\n",
        "4. For live preview use Cell under \"11. Run Live Preview\"\n",
        "5. For saved video use Cell under \"12. Save Processed Video\"\n",
        "\n",
        "---\n",
        "## ‚úÖ System Ready! üéì"
      ],
      "id": "VQ9_mfGmF0jy"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWGcxJA-gJ5t"
      },
      "id": "EWGcxJA-gJ5t",
      "execution_count": null,
      "outputs": []
    }
  ]
}