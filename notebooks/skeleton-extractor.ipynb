{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14695610,"sourceType":"datasetVersion","datasetId":9219485}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle setup\nimport sys\nsys.path.append('/kaggle/working/python_packages')\n\n# Install packages \n!pip install numpy==1.26.4 opencv-python ultralytics -t /kaggle/working/python_packages\n","metadata":{"id":"tJtBcQ6agJJ_","trusted":true,"_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/python_packages')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T16:43:20.475497Z","iopub.execute_input":"2026-01-31T16:43:20.475788Z","iopub.status.idle":"2026-01-31T16:43:20.479312Z","shell.execute_reply.started":"2026-01-31T16:43:20.475762Z","shell.execute_reply":"2026-01-31T16:43:20.478650Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom ultralytics import YOLO\n\nprint(\"NumPy:\", np.__version__)\nprint(\"OpenCV:\", cv2.__version__)\nprint(\"âœ… Ultralytics: YOLO imported successfully!\")  # ultralytics.__version__ doesn't exist\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T16:43:23.207216Z","iopub.execute_input":"2026-01-31T16:43:23.207453Z","iopub.status.idle":"2026-01-31T16:43:29.573098Z","shell.execute_reply.started":"2026-01-31T16:43:23.207433Z","shell.execute_reply":"2026-01-31T16:43:29.572430Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nNumPy: 2.0.2\nOpenCV: 4.12.0\nâœ… Ultralytics: YOLO imported successfully!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!rm -rf /kaggle/working/lstm_dataset\n!rm -rf /kaggle/working/lstm_dataset.zip\n!rm -rf  /kaggle/working/runs\n!rm -rf /kaggle/working/output\n!rm -rf /kaggle/working/skeleton-images.zip\n!rm -rf /kaggle/working/yolo11s-pose.pt\n!ls -la /kaggle/working/\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:08:38.742907Z","iopub.execute_input":"2026-01-31T17:08:38.743223Z","iopub.status.idle":"2026-01-31T17:08:40.044536Z","shell.execute_reply.started":"2026-01-31T17:08:38.743196Z","shell.execute_reply":"2026-01-31T17:08:40.043780Z"}},"outputs":[{"name":"stdout","text":"total 16\ndrwxr-xr-x   4 root root 4096 Jan 31 17:08 .\ndrwxr-xr-x   5 root root 4096 Jan 31 15:50 ..\ndrwxr-xr-x 112 root root 4096 Jan 31 15:50 python_packages\ndrwxr-xr-x   2 root root 4096 Jan 31 15:50 .virtual_documents\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Skeleton Extraction For LSTM Input\n","metadata":{}},{"cell_type":"code","source":"# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>LSTM DATA EXTRACTION<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n# ===================== IMPORTS =====================\nimport os\nimport cv2\nimport numpy as np\nimport torch\nfrom ultralytics import YOLO\nfrom collections import defaultdict\n\n# ===================== DEVICE =====================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# ===================== OUTPUT ROOT =====================\nOUTPUT_ROOT = \"/kaggle/working/lstm_dataset\"\n\n# ===================== VIDEO PATHS =====================\nVIDEO_PATHS = {\n    \"normal\": [\n        \"/kaggle/input/exam-videos/Normal_1st.mp4\",\n        \"/kaggle/input/exam-videos/Normal_2nd.mp4\",\n        \"/kaggle/input/exam-videos/Normal_3rd.mp4\",\n        \"/kaggle/input/exam-videos/Normal_4th.mp4\",\n    ],\n    \"suspicious\": [\n        \"/kaggle/input/exam-videos/Sus_1st.mp4\",\n        \"/kaggle/input/exam-videos/Sus_2nd.mp4\",\n        \"/kaggle/input/exam-videos/Sus_3rd.mp4\",\n        \"/kaggle/input/exam-videos/Sus_4th.mp4\",\n    ]\n}\n\n# ===================== PARAMETERS =====================\nIMG_SIZE = 384\nROI_SIZE = 224\nFPS_SAMPLE = 2\nKPT_CONF_TH = 0.5\n\nBODY_BONE_THICKNESS = 3\nHEAD_BONE_THICKNESS = 2\n\nMAX_STUDENTS = 3  # â† Expected number of students per video\n\n# ===================== MODEL =====================\nmodel = YOLO(\"yolo11s-pose.pt\").to(device)\n\n# ===================== COCO UPPER BODY =====================\nUPPER_BODY_KPTS = {0,1,2,3,4,5,6,7,8,9,10,11,12}\n\nSKELETON = [\n    (0,1),(0,2),(1,3),(2,4),\n    (0,5),(0,6),(5,6),\n    (5,7),(7,9),\n    (6,8),(8,10),\n    (5,11),(6,12),(11,12)\n]\n\n# ===================== VALID SKELETON CHECK =====================\ndef is_valid_skeleton(conf):\n    return (\n        conf[0] > KPT_CONF_TH and\n        conf[5] > KPT_CONF_TH and\n        conf[6] > KPT_CONF_TH and\n        (conf[11] > KPT_CONF_TH or conf[12] > KPT_CONF_TH)\n    )\n\n# ===================== ID STABILIZER =====================\nclass IDStabilizer:\n    def __init__(self, max_students=3):\n        self.max_students = max_students\n        self.id_map = {}  # original_id -> stable_id\n        self.next_stable_id = 0\n        self.id_frames = defaultdict(int)  # Count frames per ID\n    \n    def get_stable_id(self, original_id):\n        \"\"\"Map unstable tracker ID to stable student ID\"\"\"\n        if original_id not in self.id_map:\n            if self.next_stable_id < self.max_students:\n                self.id_map[original_id] = self.next_stable_id\n                self.next_stable_id += 1\n            else:\n                # If we have more IDs than expected, map to most similar existing ID\n                # For now, just reject extra IDs\n                return None\n        \n        self.id_frames[original_id] += 1\n        return self.id_map[original_id]\n    \n    def get_stats(self):\n        \"\"\"Return statistics about ID usage\"\"\"\n        return {\n            'total_original_ids': len(self.id_map),\n            'stable_ids_used': len(set(self.id_map.values())),\n            'frames_per_id': dict(self.id_frames)\n        }\n\n# ===================== PROCESS =====================\nfor label, videos in VIDEO_PATHS.items():\n    for v_idx, video_path in enumerate(videos):\n\n        if not os.path.exists(video_path):\n            print(f\"âŒ Missing video: {video_path}\")\n            continue\n\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            print(f\"âŒ Failed to open {video_path}\")\n            continue\n\n        # Initialize ID stabilizer for this video\n        id_stabilizer = IDStabilizer(max_students=MAX_STUDENTS)\n\n        fps = int(cap.get(cv2.CAP_PROP_FPS))\n        frame_gap = max(1, fps // FPS_SAMPLE)\n        frame_idx = 0\n        saved = 0\n\n        save_root = os.path.join(\n            OUTPUT_ROOT,\n            label,\n            f\"video_{v_idx+1:02d}\"\n        )\n        os.makedirs(save_root, exist_ok=True)\n\n        print(f\"\\nðŸŽ¥ Processing {label}/video_{v_idx+1:02d} @ {FPS_SAMPLE} FPS\")\n\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            frame_idx += 1\n            if frame_idx % frame_gap != 0:\n                continue\n\n            results = model.track(\n                frame,\n                imgsz=IMG_SIZE,\n                conf=0.4,\n                persist=True,\n                device=device,\n                verbose=False,\n                tracker=\"botsort.yaml\"  # â† Better tracker for crowded scenes\n            )[0]\n\n            if results.keypoints is None or results.boxes.id is None:\n                continue\n\n            kpts_xy = results.keypoints.xy.cpu().numpy()\n            kpts_conf = results.keypoints.conf.cpu().numpy()\n            track_ids = results.boxes.id.cpu().numpy().astype(int)\n\n            for tid, xy, conf in zip(track_ids, kpts_xy, kpts_conf):\n\n                if not is_valid_skeleton(conf):\n                    continue\n\n                # Get stable ID\n                stable_id = id_stabilizer.get_stable_id(tid)\n                if stable_id is None:\n                    continue  # Skip extra IDs beyond MAX_STUDENTS\n\n                valid_pts = [xy[i] for i in UPPER_BODY_KPTS if conf[i] > KPT_CONF_TH]\n                if len(valid_pts) < 4:\n                    continue\n\n                pts = np.array(valid_pts)\n                min_x, min_y = pts.min(axis=0)\n                max_x, max_y = pts.max(axis=0)\n\n                scale = 0.9 * ROI_SIZE / max(max_x - min_x, max_y - min_y)\n                cx = (min_x + max_x) / 2\n                cy = (min_y + max_y) / 2\n\n                skeleton_img = np.zeros((ROI_SIZE, ROI_SIZE), dtype=np.uint8)\n\n                def norm(pt):\n                    x = (pt[0] - cx) * scale + ROI_SIZE / 2\n                    y = (pt[1] - cy) * scale + ROI_SIZE / 2\n                    return int(x), int(y)\n\n                for i, j in SKELETON:\n                    if conf[i] < KPT_CONF_TH or conf[j] < KPT_CONF_TH:\n                        continue\n\n                    p1, p2 = norm(xy[i]), norm(xy[j])\n                    if not (0 <= p1[0] < ROI_SIZE and 0 <= p1[1] < ROI_SIZE and\n                            0 <= p2[0] < ROI_SIZE and 0 <= p2[1] < ROI_SIZE):\n                        continue\n\n                    thickness = HEAD_BONE_THICKNESS if (i == 0 or j == 0) else BODY_BONE_THICKNESS\n                    cv2.line(skeleton_img, p1, p2, 255, thickness)\n\n                skeleton_img = cv2.dilate(\n                    skeleton_img, np.ones((3,3), np.uint8), iterations=1\n                )\n\n                student_dir = os.path.join(save_root, f\"student_{stable_id}\")\n                os.makedirs(student_dir, exist_ok=True)\n\n                cv2.imwrite(\n                    os.path.join(student_dir, f\"f{frame_idx:06d}.png\"),\n                    skeleton_img\n                )\n                saved += 1\n\n        cap.release()\n        \n        # Print ID statistics\n        stats = id_stabilizer.get_stats()\n        print(f\"âœ… Saved {saved} skeleton frames\")\n        print(f\"ðŸ“Š ID Stats: {stats['total_original_ids']} original IDs â†’ {stats['stable_ids_used']} stable IDs\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"âœ… SKELETON DATASET COLLECTION COMPLETE\")\nprint(f\"ðŸ“ Output: {OUTPUT_ROOT}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T14:03:09.260173Z","iopub.execute_input":"2026-02-01T14:03:09.260607Z","iopub.status.idle":"2026-02-01T14:03:12.856759Z","shell.execute_reply.started":"2026-02-01T14:03:09.260566Z","shell.execute_reply":"2026-02-01T14:03:12.854970Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3227441925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2191\u001b[0m )\n\u001b[1;32m   2192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2193\u001b[0;31m from torch import (\n\u001b[0m\u001b[1;32m   2194\u001b[0m     \u001b[0m__config__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__config__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m     \u001b[0m__future__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__future__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"# Make lstm_dataset.zip visible in kaggle/working to download the dataset","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\n\nSRC_DIR = \"/kaggle/working/lstm_dataset\"\nZIP_PATH = \"/kaggle/working/lstm_dataset\"  # This will create lstm_dataset.zip\n\n# Check if source directory exists\nassert os.path.exists(SRC_DIR), f\"Directory not found: {SRC_DIR}\"\n\n# Create zip archive\nshutil.make_archive(ZIP_PATH, 'zip', SRC_DIR)\n\nprint(\"âœ… Zip complete! Download 'lstm_dataset.zip' from Output tab\")\nprint(f\"ðŸ“¦ Zip file location: {ZIP_PATH}.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:18:14.753892Z","iopub.execute_input":"2026-01-31T17:18:14.754397Z","iopub.status.idle":"2026-01-31T17:18:16.955837Z","shell.execute_reply.started":"2026-01-31T17:18:14.754366Z","shell.execute_reply":"2026-01-31T17:18:16.955079Z"}},"outputs":[{"name":"stdout","text":"âœ… Zip complete! Download 'lstm_dataset.zip' from Output tab\nðŸ“¦ Zip file location: /kaggle/working/lstm_dataset.zip\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Skeleton Extraction For ResNet50v2 Input ","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nfrom ultralytics import YOLO\n\n# ================== KAGGLE-SPECIFIC MODIFICATIONS ==================\n# Set Kaggle working directory\nos.chdir('/kaggle/working')\n\n# ================== PARAMETERS ==================\nIMG_SIZE = 384          # YOLO pose input\nROI_SIZE = 224          # ResNet input\nFRAME_SKIP = 5\nKPT_CONF_TH = 0.5\n\nJOINT_RADIUS = 3        # (kept but NOT USED â€“ explicitly)\nBODY_BONE_THICKNESS = 3\nHEAD_BONE_THICKNESS = 2\n\n# Updated path for Kaggle input structure\nVIDEO_PATH = \"/kaggle/input/exam-videos/Normal_1st.mp4\"\n\n# Use Kaggle's working directory for output\nOUTPUT_DIR = \"/kaggle/working/skeletons\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# ================== DEVICE ==================\n# Kaggle has GPU available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# ================== MODEL ==================\n# Load model directly - Kaggle has internet access for downloads\nmodel = YOLO(\"yolov8s-pose.pt\").to(device)\n\n# ================== COCO UPPER-BODY KEYPOINTS ==================\nUPPER_BODY_KPTS = {0,1,2,3,4,5,6,7,8,9,10,11,12}\n\n# ================== UPPER-BODY SKELETON ==================\nSKELETON = [\n    (0,1),(0,2),(1,3),(2,4),      # head\n    (0,5),(0,6),(5,6),            # neck / shoulders\n    (5,7),(7,9),                  # left arm\n    (6,8),(8,10),                 # right arm\n    (5,11),(6,12),(11,12)         # torso\n]\n\n# ================== SKELETON VALIDATION ==================\ndef is_valid_skeleton(conf, th):\n    return (\n        conf[0] > th and\n        conf[5] > th and conf[6] > th and\n        (conf[11] > th or conf[12] > th)\n    )\n\n# ================== VIDEO ==================\ncap = cv2.VideoCapture(VIDEO_PATH)\nframe_idx = 0\nsaved_count = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    frame_idx += 1\n    if frame_idx % FRAME_SKIP != 0:\n        continue\n\n    results = model.track(\n        frame,\n        imgsz=IMG_SIZE,\n        conf=0.4,\n        persist=True,\n        device=device,\n        verbose=False\n    )[0]\n\n    if results.keypoints is None:\n        continue\n\n    kpts_xy = results.keypoints.xy.cpu().numpy()\n    kpts_conf = results.keypoints.conf.cpu().numpy()\n    boxes = results.boxes.xyxy.cpu().numpy()\n\n    for pid, (xy, conf, box) in enumerate(zip(kpts_xy, kpts_conf, boxes)):\n\n        if not is_valid_skeleton(conf, KPT_CONF_TH):\n            continue\n\n        # ================== USE ONLY UPPER BODY POINTS ==================\n        valid_pts = [\n            xy[i] for i in UPPER_BODY_KPTS if conf[i] > KPT_CONF_TH\n        ]\n\n        if len(valid_pts) < 4:\n            continue\n\n        pts = np.array(valid_pts)\n        min_x, min_y = pts.min(axis=0)\n        max_x, max_y = pts.max(axis=0)\n\n        skel_w = max_x - min_x\n        skel_h = max_y - min_y\n        scale = 0.9 * ROI_SIZE / max(skel_w, skel_h)\n\n        cx = (min_x + max_x) / 2\n        cy = (min_y + max_y) / 2\n\n        skeleton_img = np.zeros((ROI_SIZE, ROI_SIZE), dtype=np.uint8)\n\n        def norm(pt):\n            x = (pt[0] - cx) * scale + ROI_SIZE / 2\n            y = (pt[1] - cy) * scale + ROI_SIZE / 2\n            return int(x), int(y)\n\n        # ================== DRAW BONES ONLY ==================\n        for i, j in SKELETON:\n            if conf[i] < KPT_CONF_TH or conf[j] < KPT_CONF_TH:\n                continue\n\n            if i not in UPPER_BODY_KPTS or j not in UPPER_BODY_KPTS:\n                continue\n\n            p1 = norm(xy[i])\n            p2 = norm(xy[j])\n\n            if not (0 <= p1[0] < ROI_SIZE and 0 <= p1[1] < ROI_SIZE and\n                    0 <= p2[0] < ROI_SIZE and 0 <= p2[1] < ROI_SIZE):\n                continue\n\n            thickness = HEAD_BONE_THICKNESS if (i == 0 or j == 0) else BODY_BONE_THICKNESS\n            cv2.line(skeleton_img, p1, p2, 255, thickness)\n\n        # ================== POSTPROCESS ==================\n        kernel = np.ones((3,3), np.uint8)\n        skeleton_img = cv2.dilate(skeleton_img, kernel, iterations=1)\n\n        filename = f\"f{frame_idx:05d}_p{pid:02d}.png\"\n        cv2.imwrite(os.path.join(OUTPUT_DIR, filename), skeleton_img)\n        saved_count += 1\n\n    print(f\"Processed frame {frame_idx}\")\n\ncap.release()\nprint(f\"âœ… DONE. Saved {saved_count} CLEAN upper-body skeleton images.\")\nprint(f\"Output saved to: {OUTPUT_DIR}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ZIP the dataset folder to download","metadata":{}},{"cell_type":"code","source":"# At the end of your code, compress the output directory\nimport shutil\nshutil.make_archive('/kaggle/working/skeletons_output', 'zip', '/kaggle/working/skeletons')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}