{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b934a78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#>>>>>>>>>>>>>>FINE TUNING BOTH RESNET50V2 AND LSTM SIMULTANEOUSLY<<<<<\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ================================\n",
    "# 1️⃣ CONFIG\n",
    "# ================================\n",
    "DATASET_PATH = \"/content/drive/MyDrive/skeleton_dataset\"  # new/updated dataset\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 4\n",
    "TIMESTEPS = 16\n",
    "NUM_CLASSES = 4\n",
    "EPOCHS_CONTINUE = 5\n",
    "EPOCHS_CONTINUE_2 = 5  # adjust based on dataset size\n",
    "\n",
    "# Class mapping\n",
    "class_indices = {'body':0, 'hand':1, 'head':2, 'normal':3}\n",
    "class_names = list(class_indices.keys())\n",
    "\n",
    "# ================================\n",
    "# 2️⃣ CUSTOM SEQUENCE GENERATOR\n",
    "# ================================\n",
    "class SkeletonSequence(Sequence):\n",
    "    def __init__(self, folder_path, timesteps=TIMESTEPS, batch_size=BATCH_SIZE, shuffle=True):\n",
    "        self.folder_path = folder_path\n",
    "        self.timesteps = timesteps\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.samples = []\n",
    "\n",
    "        for class_name, idx in class_indices.items():\n",
    "            class_folder = os.path.join(folder_path, class_name)\n",
    "            images = sorted(os.listdir(class_folder))\n",
    "            random.shuffle(images)  # shuffle images to avoid order bias\n",
    "\n",
    "            # sliding window with stride=1 to maximize data usage\n",
    "        stride = TIMESTEPS  # or 1 if you want maximum sequences\n",
    "        for i in range(0, len(images) - timesteps + 1, stride):\n",
    "            seq_files = images[i:i+timesteps]\n",
    "            self.samples.append((seq_files, class_name))\n",
    "\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.samples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_samples = self.samples[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        X = np.zeros((len(batch_samples), self.timesteps, IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "        y = np.zeros((len(batch_samples), NUM_CLASSES), dtype=np.float32)\n",
    "\n",
    "        for i, (seq_files, class_name) in enumerate(batch_samples):\n",
    "            for t, fname in enumerate(seq_files):\n",
    "                img_path = os.path.join(self.folder_path, class_name, fname)\n",
    "                img = load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "                img_array = img_to_array(img)\n",
    "                img_array = tf.keras.applications.resnet_v2.preprocess_input(img_array)\n",
    "                X[i, t] = img_array\n",
    "\n",
    "            y[i] = to_categorical(class_indices[class_name], NUM_CLASSES)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.samples)\n",
    "\n",
    "# ================================\n",
    "# 3️⃣ LOAD TRAIN/VAL GENERATORS\n",
    "# ================================\n",
    "all_seq = SkeletonSequence(DATASET_PATH, shuffle=True)\n",
    "split = int(0.8 * len(all_seq.samples))\n",
    "\n",
    "train_gen = SkeletonSequence(DATASET_PATH, shuffle=True)\n",
    "train_gen.samples = all_seq.samples[:split]\n",
    "\n",
    "val_gen = SkeletonSequence(DATASET_PATH, shuffle=False)\n",
    "val_gen.samples = all_seq.samples[split:]\n",
    "\n",
    "print(f\"Train sequences: {len(train_gen.samples)}, Val sequences: {len(val_gen.samples)}\")\n",
    "\n",
    "# ================================\n",
    "# 4️⃣ LOAD PRETRAINED MODELS\n",
    "# ================================\n",
    "# Load ResNet50V2\n",
    "cnn = load_model(\"/content/drive/MyDrive/resnet50v2_best.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4dd0a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Stage 1: Fine-tune last 10 layers of CNN\n",
    "# ================================\n",
    "\n",
    "# Freeze all layers except last 10\n",
    "for layer in cnn.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "for layer in cnn.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile model for Stage 1\n",
    "end_to_end_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),  # slightly higher LR for head layers\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks_stage1 = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor='val_loss', verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"/content/drive/MyDrive/resnet50v2_lstm_stage1.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train Stage 1\n",
    "history_stage1 = end_to_end_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS_CONTINUE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks_stage1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb5059",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Final Cell: Combine Stage 1 + Stage 2 histories\n",
    "# ================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine Stage 1 and Stage 2 metrics\n",
    "loss = history_stage1.history['loss'] + history_stage2.history['loss']\n",
    "val_loss = history_stage1.history['val_loss'] + history_stage2.history['val_loss']\n",
    "accuracy = history_stage1.history['accuracy'] + history_stage2.history['accuracy']\n",
    "val_accuracy = history_stage1.history['val_accuracy'] + history_stage2.history['val_accuracy']\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(loss, label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0].plot(val_loss, label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0].set_title('Fine-tuning Loss (Stage1+Stage2)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(accuracy, label='Train Accuracy', marker='o', linewidth=2)\n",
    "axes[1].plot(val_accuracy, label='Val Accuracy', marker='s', linewidth=2)\n",
    "axes[1].set_title('Fine-tuning Accuracy (Stage1+Stage2)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/fine_tuning_combined_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
